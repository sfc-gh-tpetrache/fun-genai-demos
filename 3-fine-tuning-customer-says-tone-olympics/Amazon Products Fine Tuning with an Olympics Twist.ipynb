{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "collapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport altair as alt\nimport snowflake.snowpark.functions as F\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "01a4b9a5-b7ae-46b0-857d-e979cd9d948f",
   "metadata": {
    "language": "sql",
    "name": "preview",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM MUSICAL_INSTRUMENTS_REVIEWS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71783dbc-c9ee-44d1-b84a-659df540e432",
   "metadata": {
    "language": "sql",
    "name": "save_preprocess",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE summarized_reviews AS\nWITH concatenated_reviews AS (\n    SELECT asin, \n           LISTAGG(reviewText, ' ') AS concatenated_review_text,\n           AVG(OVERALL) AS AVG_SCORE, \n           AVG(SNOWFLAKE.CORTEX.SENTIMENT(reviewText)) AS AVG_SENTIMENT\n    FROM MUSICAL_INSTRUMENTS_REVIEWS \n    GROUP BY asin\n)\nSELECT asin, \n       AVG_SCORE,\n       AVG_SENTIMENT, \n       concatenated_review_text\nFROM concatenated_reviews",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3cedff6-ec11-4bae-b84c-da99f66139db",
   "metadata": {
    "language": "sql",
    "name": "preview_summary",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM summarized_reviews LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65dd5827-923c-4b79-8d30-3112baf99f12",
   "metadata": {
    "language": "sql",
    "name": "cortex_summarize",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS('SUMMARIZE', concatenated_review_text) AS CNT_TOKENS,\n       SNOWFLAKE.CORTEX.SUMMARIZE(concatenated_review_text) AS summary\nFROM summarized_reviews LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ec2adc7-150a-4c65-9d49-7764ba99a7b9",
   "metadata": {
    "language": "python",
    "name": "histograms",
    "collapsed": false
   },
   "outputs": [],
   "source": "df = session.table(\"summarized_reviews\");\n\ndf_pandas = df.to_pandas()\n# Generate the first histogram for avg_Score\nhistogram1 = alt.Chart(df_pandas).mark_bar().encode(\n    alt.X('AVG_SCORE:Q', bin=alt.Bin(maxbins=10), title='Average Score'),\n    alt.Y('count()', title='Frequency')\n).properties(\n    title='Histogram of Avg Score Reviews'\n)\n\n# Generate the second histogram for another_Score\nhistogram2 = alt.Chart(df_pandas).mark_bar().encode(\n    alt.X('AVG_SENTIMENT:Q', bin=alt.Bin(maxbins=10), title='Average Sentiment'),\n    alt.Y('count()', title='Frequency')\n).properties(\n    title='Histogram of Average Sentiment'\n)\n\n# Concatenate the two histograms horizontally\ncombined_histograms = alt.hconcat(histogram1, histogram2)\n\n# Use Streamlit to display the combined histograms\nst.altair_chart(combined_histograms, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6ebc2ed-a7c5-4ca9-b981-51d695154002",
   "metadata": {
    "language": "python",
    "name": "prompt",
    "collapsed": false
   },
   "outputs": [],
   "source": "prompt_text = \"\"\"You are a sports commentator at the Olympic Games. Your task is to generate one-paragraph summaries of Amazon product reviews, capturing the excitement, energy, and competitive spirit of a sports event.\n\nGuidelines:\n- Summarize the reviews in a single paragraph.\n- Use a tone that conveys enthusiasm, energy, and a sense of competition.\n- Use sports-related metaphors and analogies to describe the product features and performance.\n- Ensure the summary is clear, concise, and engaging.\"\"\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bd9a27c-c171-4c08-bcdb-8986121e6cc1",
   "metadata": {
    "language": "sql",
    "name": "preview_mistral_large",
    "collapsed": false
   },
   "outputs": [],
   "source": "select trim(snowflake.cortex.complete('mistral-large',concat('{prompt}',concatenated_review_text)),'\\n') as llm_response\nfrom summarized_reviews limit 3;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26761fb9-10bf-4be5-b0ba-fc1120f37dba",
   "metadata": {
    "language": "sql",
    "name": "prep_data_finetuning",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE REVIEWS4FINETUNING AS\nSELECT\n f.asin, s.concatenated_review_text, f.exciting_summary\nFROM\n  AMAZON_REVIEWS.PUBLIC.AMAZON_REVIEWS_FINETUNE f\nINNER JOIN summarized_reviews s\nON s.asin = f.asin",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26203b1c-b212-4722-a699-952d5944007c",
   "metadata": {
    "language": "sql",
    "name": "preview_finetuning_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM REVIEWS4FINETUNING LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f88f62f-04a2-4bd4-8e28-48c9eb86355e",
   "metadata": {
    "language": "python",
    "name": "split_train_test",
    "collapsed": false
   },
   "outputs": [],
   "source": "df_fine_tune = session.table(\"REVIEWS4FINETUNING\");\ndf_fine_tune_prompt = df_fine_tune.with_column(\"prompt\", F.concat(F.lit(prompt_text),F.lit(\" \"),F.col(\"concatenated_review_text\"))).select(\"asin\",\"prompt\",\"exciting_summary\")\n\ntrain_df, eval_df = df_fine_tune_prompt.random_split(weights=[0.8, 0.2], seed=42)\nprint(train_df.count(), eval_df.count())\ntrain_df.write.mode('overwrite').save_as_table('reviews_train')\neval_df.write.mode('overwrite').save_as_table('reviews_eval')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfb301a7-7ccd-4736-aaa6-58a458a91fd5",
   "metadata": {
    "language": "python",
    "name": "preview_training_set",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.table('reviews_train').show(1)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9db2cb82-9804-4b4f-970f-1fe859abbedf",
   "metadata": {
    "name": "start_fine_tuning",
    "collapsed": false
   },
   "source": "##  Fine-tune mistral 7b using Cortex\n"
  },
  {
   "cell_type": "code",
   "id": "49b399ca-1780-47ad-a2b8-db94a06f5c4a",
   "metadata": {
    "language": "sql",
    "name": "cortex_finetune",
    "collapsed": false
   },
   "outputs": [],
   "source": "select snowflake.cortex.finetune('CREATE', 'AMAZON_REVIEWS_FINETUNED_MISTRAL_7B', 'mistral-7b', 'SELECT prompt, exciting_summary as completion from reviews_train', 'SELECT prompt, exciting_summary as completion from reviews_eval');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0453ac2f-21cd-406b-b63a-cefa05764aee",
   "metadata": {
    "language": "sql",
    "name": "fine_tune_status",
    "collapsed": false
   },
   "outputs": [],
   "source": "select snowflake.cortex.finetune('DESCRIBE', 'CortexFineTuningWorkflow_eeb61b0f-6ffa-469e-a762-2550467154e8');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f14a8872-99b7-4f62-b3cd-e1288bd749f8",
   "metadata": {
    "language": "python",
    "name": "inference",
    "collapsed": false
   },
   "outputs": [],
   "source": "fine_tuned_model_name = 'AMAZON_REVIEWS_FINETUNED_MISTRAL_7B'\nsql_text = f\"\"\"\nSELECT asin, concatenated_review_text,\n       TRIM(snowflake.cortex.complete('{fine_tuned_model_name}', CONCAT('{prompt_text}', concatenated_review_text)), '\\n') AS fine_tuned_mistral_7b_model_response\nFROM summarized_reviews\n\"\"\"\n\ndf_fine_tuned_mistral_7b_response = session.sql(sql_text)\ndf_fine_tuned_mistral_7b_response.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77e22fcf-aaff-42b6-b78a-63c6626da725",
   "metadata": {
    "language": "python",
    "name": "save_predictions",
    "collapsed": false
   },
   "outputs": [],
   "source": "df_fine_tuned_mistral_7b_response.write.mode('overwrite').save_as_table('reviews_generated_mistral_finetuned')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f19d6ad-7bcb-472c-8061-e960c61a2741",
   "metadata": {
    "language": "sql",
    "name": "preview_results"
   },
   "outputs": [],
   "source": "SELECT * FROM reviews_generated_mistral_finetuned LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48c7de7a-4427-48cf-866c-9604ea1e7659",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Apply *ROUGE (Recall-Oriented Understudy for Gisting Evaluation)* scores as a metric for evaluating summarization tasks with large language models (LLMs)"
  },
  {
   "cell_type": "code",
   "id": "45da9a30-cadc-48a2-a962-9457140efb65",
   "metadata": {
    "language": "python",
    "name": "rouge_score",
    "collapsed": false
   },
   "outputs": [],
   "source": "from rouge_score import rouge_scorer\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n# Load the tables\ndf_generated_summaries = session.table(\"reviews_generated_mistral_finetuned\")\ndf_orig_reviews = session.table(\"REVIEWS4FINETUNING\")\n\ndf_joined = df_generated_summaries.join(df_orig_reviews, df_generated_summaries[\"asin\"] == df_orig_reviews[\"asin\"])\n\n# Collect the joined DataFrame to the client\njoined_data = df_joined.select(\"EXCITING_SUMMARY\", \"FINE_TUNED_MISTRAL_7B_MODEL_RESPONSE\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "116abde8-6271-4fa0-aad3-8d2fc1d49d21",
   "metadata": {
    "language": "python",
    "name": "evaluate_rouge_scores",
    "collapsed": false
   },
   "outputs": [],
   "source": "# reference versus generated summaries\n\nrouge_scores = []\nfor row in joined_data:\n    ref = row[\"EXCITING_SUMMARY\"]\n    gen = row[\"FINE_TUNED_MISTRAL_7B_MODEL_RESPONSE\"]\n    scores = scorer.score(ref, gen)\n    rouge_scores.append({\n        'ROUGE-1': scores['rouge1'].fmeasure,\n        'ROUGE-2': scores['rouge2'].fmeasure,\n        'ROUGE-L': scores['rougeL'].fmeasure\n    })\n\n# Convert the list to a DataFrame\ndf_rouge_scores = pd.DataFrame(rouge_scores)\n\n# Create Altair charts\nchart_rouge1 = alt.Chart(df_rouge_scores).mark_bar().encode(\n    x=alt.X('ROUGE-1:Q', bin=True, title='ROUGE-1 Score'),\n    y=alt.Y('count()', title='Count')\n).properties(\n    title='ROUGE-1 Scores'\n)\n\nchart_rouge2 = alt.Chart(df_rouge_scores).mark_bar().encode(\n    x=alt.X('ROUGE-2:Q', bin=True, title='ROUGE-2 Score'),\n    y=alt.Y('count()', title='Count')\n).properties(\n    title='ROUGE-2 Scores'\n)\n\nchart_rougeL = alt.Chart(df_rouge_scores).mark_bar().encode(\n    x=alt.X('ROUGE-L:Q', bin=True, title='ROUGE-L Score'),\n    y=alt.Y('count()', title='Count')\n).properties(\n    title='ROUGE-L Scores'\n)\n\n# Combine charts side by side\ncombined_chart = alt.hconcat(chart_rouge1, chart_rouge2, chart_rougeL).resolve_scale(y='independent')\n\n# Display the chart\nst.altair_chart(combined_chart, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0d5c2586-16b5-41a7-a1df-f45a25e76201",
   "metadata": {
    "name": "interpret_rouge_score",
    "collapsed": false
   },
   "source": "**ROUGE-1**\n\nDefinition: Measures the overlap of unigrams (individual words) between the generated summary and the reference summary.\nInterpretation: A higher ROUGE-1 score indicates that the generated summary has a higher overlap of words with the reference summary, suggesting it captures more of the important content.   \nGood Value: A ROUGE-1 score above 0.5 is generally considered good\n\n**ROUGE-2**\n\nDefinition: Measures the overlap of bigrams (pairs of consecutive words) between the generated summary and the reference summary.\nInterpretation: A higher ROUGE-2 score indicates that the generated summary preserves more of the sequence of words found in the reference summary, reflecting better fluency and coherence.   \nGood Value: A ROUGE-2 score above 0.3 is typically seen as good.\n\n**ROUGE-L**\n\nDefinition: Measures the longest common subsequence (LCS) between the generated summary and the reference summary.\nInterpretation: A higher ROUGE-L score indicates that the generated summary has a longer sequence of words in common with the reference summary, suggesting better structural similarity and more comprehensive coverage of the important content.   \nGood Value: A ROUGE-L score above 0.4 is considered good"
  }
 ]
}